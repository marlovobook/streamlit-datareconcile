{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from athena_to_pd import athena_query\n",
    "import sqlite3\n",
    "#By Book\n",
    "from pathlib import Path\n",
    "import os\n",
    "import awswrangler as wr \n",
    "import boto3\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd = os.getcwd()\n",
    "\n",
    "PROFILE = \"vej-book\"\n",
    "DATABASE = \"vtnlake_dev_curated_finance\" \n",
    "#SQLITE_PATH = \"../vtnlake_onedrive.db\"\n",
    "SQLITE_PATH = rf'D:\\Work\\SQLite\\vtnlake_onedrive.db' # Book's Labtop\n",
    "#SQLITE_PATH = rf'E:\\Work\\TradeSquare\\SQlite Test\\29102024\\vtnlake_onedrive.db' # Book's PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_filter = athena_query(PROFILE, DATABASE,\"\"\"\n",
    "    select \n",
    "        min(date_format(parse_datetime(_file_updated_date,'yyyy-MM-dd''T''HH:mm:ss''Z'),'%Y-%m-%d %H:%i:%s')) AS min_date,\n",
    "        max(date_format(parse_datetime(_file_updated_date,'yyyy-MM-dd''T''HH:mm:ss''Z'),'%Y-%m-%d %H:%i:%s')) AS max_date\n",
    "        \n",
    "    from gl_header\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Date Filter:  2024-11-04 13:29:23\n",
      "Max Date Filter:  2024-11-05 01:45:11\n"
     ]
    }
   ],
   "source": [
    "df_date_filter = pd.DataFrame(date_filter, columns = date_filter.columns)\n",
    "min_date = df_date_filter.iloc[0, 0]\n",
    "max_date = df_date_filter.iloc[0, 1]\n",
    "\n",
    "print(\"Min Date Filter: \", min_date)\n",
    "print(\"Max Date Filter: \", max_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Select DISTINCT script for Athena (only PK)\n",
    "'''\n",
    "\n",
    "# Read the SQL template file (Book's PC)\n",
    "with open(rf\"{pwd}/sql_template/gl_header/select_dis_gl_header_athena.sql\", \"r\") as file:\n",
    "    query_template = file.read()\n",
    "\n",
    "# Substitute placeholders with actual values\n",
    "query_gl_header = query_template.format(min_date=min_date, max_date=max_date)\n",
    "\n",
    "# Run the query with wr\n",
    "select_gl_header_athena = wr.athena.read_sql_query(sql=query_gl_header, \n",
    "        database=DATABASE,\n",
    "        boto3_session=boto3.session.Session(profile_name=PROFILE, region_name='ap-southeast-1')                      \n",
    "                              )\n",
    "\n",
    "##---------------------------------##\n",
    "#global variables for sqlite\n",
    "TABLE = \"headers_full\"\n",
    "\n",
    "'''\n",
    "Select DISTINCT script for SQLite (only PK)\n",
    "'''\n",
    "\n",
    "# Read the SQL template file\n",
    "with open(rf\"{pwd}/sql_template/gl_header/select_dis_gl_header_sqlite.sql\", \"r\") as file:\n",
    "    query_template = file.read()\n",
    "\n",
    "# Substitute placeholders with actual values\n",
    "SQLITE_QUERY = query_template.format(\n",
    "    TABLE=TABLE,\n",
    "    min_date=min_date,\n",
    "    max_date=max_date\n",
    ")    \n",
    "# Execute the query\n",
    "conn = sqlite3.connect(SQLITE_PATH)\n",
    "df_select_sqlite = pd.read_sql_query(SQLITE_QUERY, conn).add_suffix('_sqlite')\n",
    "conn.close()\n",
    "select_gl_header_sqlite = df_select_sqlite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sqlite\n",
    "select_gl_header_sqlite.sort_values(by=['invoice_no_receipt_no_sqlite', 'payment_method_sqlite'], inplace=True)\n",
    "select_gl_header_sqlite.reset_index(drop=True, inplace=True)\n",
    "#athena\n",
    "select_gl_header_athena.sort_values(by=['invoice_no_receipt_no', 'payment_method'], inplace=True)\n",
    "select_gl_header_athena.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#Concat dataframe for Reconcilation\n",
    "df_reconcile = pd.concat([select_gl_header_sqlite, select_gl_header_athena], axis=1)\n",
    "\n",
    "#check status if suffix sqlite is match with athena using lambda for each column and row\n",
    "df_reconcile['invoice_no_receipt_no_status'] = df_reconcile.apply(lambda x: 'Match' if x['invoice_no_receipt_no_sqlite'] == x['invoice_no_receipt_no'] else 'Mismatch', axis=1)\n",
    "df_reconcile['payment_method_status'] = df_reconcile.apply(lambda x: 'Match' if x['payment_method_sqlite'] == x['payment_method'] else 'Mismatch', axis=1)\n",
    "df_reconcile['status'] = df_reconcile.apply(lambda x: 'Match' if x['invoice_no_receipt_no_sqlite'] == x['invoice_no_receipt_no'] and x['payment_method_sqlite'] == x['payment_method'] else 'Mismatch', axis=1)\n",
    "\n",
    "# df_reconcile[df_reconcile['status'] == 'Mismatch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_gl_header_sqlite =  pd.DataFrame(select_gl_header_sqlite.describe(include='all'))\n",
    "\n",
    "\n",
    "# Convert all string values to hashed integers in a copy of the DataFrame\n",
    "hashed_df = select_gl_header_sqlite.apply(lambda col: col.map(lambda x: hash(x) if isinstance(x, str) else x))\n",
    "\n",
    "'''\n",
    "SUM HASHED\n",
    "'''\n",
    "# Calculate the sum of each column and convert to a DataFrame row\n",
    "sum_gl_header_sqlite = hashed_df.sum().to_frame().T\n",
    "sum_gl_header_sqlite.index = [\"sum_hashed\"] \n",
    "\n",
    "'''\n",
    "AVG HASHED\n",
    "'''\n",
    "avg_gl_header_sqlite = hashed_df.mean().to_frame().T\n",
    "avg_gl_header_sqlite.index = [\"avg_hashed\"]\n",
    "\n",
    "'''\n",
    "MIN HASHED\n",
    "'''\n",
    "min_gl_header_sqlite = hashed_df.min().to_frame().T\n",
    "min_gl_header_sqlite.index = [\"min_hashed\"]\n",
    "\n",
    "'''\n",
    "MAX HASHED\n",
    "'''\n",
    "max_gl_header_sqlite = hashed_df.max().to_frame().T\n",
    "max_gl_header_sqlite.index = [\"max_hashed\"]\n",
    "\n",
    "# Concatenate the statistics DataFrames\n",
    "stat_select_gl_header_sqlite = pd.concat([stat_gl_header_sqlite, sum_gl_header_sqlite, avg_gl_header_sqlite, min_gl_header_sqlite, max_gl_header_sqlite], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Athena Statistics\n",
    "\n",
    "stat_gl_header_athena =  pd.DataFrame(select_gl_header_athena.describe(include='all'))\n",
    "\n",
    "\n",
    "# Convert all string values to hashed integers in a copy of the DataFrame\n",
    "hashed_df = select_gl_header_athena.apply(lambda col: col.map(lambda x: hash(x) if isinstance(x, str) else x))\n",
    "\n",
    "'''\n",
    "SUM HASHED\n",
    "'''\n",
    "# Calculate the sum of each column and convert to a DataFrame row\n",
    "sum_gl_header_athena = hashed_df.sum().to_frame().T\n",
    "sum_gl_header_athena.index = [\"sum_hashed\"] \n",
    "\n",
    "'''\n",
    "AVG HASHED\n",
    "'''\n",
    "avg_gl_header_athena = hashed_df.mean().to_frame().T\n",
    "avg_gl_header_athena.index = [\"avg_hashed\"]\n",
    "\n",
    "'''\n",
    "MIN HASHED\n",
    "'''\n",
    "min_gl_header_athena = hashed_df.min().to_frame().T\n",
    "min_gl_header_athena.index = [\"min_hashed\"]\n",
    "\n",
    "'''\n",
    "MAX HASHED\n",
    "'''\n",
    "max_gl_header_athena = hashed_df.max().to_frame().T\n",
    "max_gl_header_athena.index = [\"max_hashed\"]\n",
    "\n",
    "# Concatenate the statistics DataFrames\n",
    "stat_select_gl_header_athena = pd.concat([stat_gl_header_athena, sum_gl_header_athena, avg_gl_header_athena, min_gl_header_athena, max_gl_header_athena], axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>invoice_no_receipt_no</th>\n",
       "      <th>payment_method</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>525326</td>\n",
       "      <td>525326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>499211</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>D66016691</td>\n",
       "      <td>CRC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4</td>\n",
       "      <td>172947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum_hashed</th>\n",
       "      <td>-8217080412292853933</td>\n",
       "      <td>-7384661524365760264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_hashed</th>\n",
       "      <td>-8197402088582363.0</td>\n",
       "      <td>-1778721706085759488.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_hashed</th>\n",
       "      <td>-9223370346243070901</td>\n",
       "      <td>-9044453527849431452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_hashed</th>\n",
       "      <td>9223318366835145502</td>\n",
       "      <td>7243502204763661199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           invoice_no_receipt_no         payment_method\n",
       "count                     525326                 525326\n",
       "unique                    499211                     11\n",
       "top                    D66016691                    CRC\n",
       "freq                           4                 172947\n",
       "sum_hashed  -8217080412292853933   -7384661524365760264\n",
       "avg_hashed   -8197402088582363.0 -1778721706085759488.0\n",
       "min_hashed  -9223370346243070901   -9044453527849431452\n",
       "max_hashed   9223318366835145502    7243502204763661199"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat_select_gl_header_athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare statistics between SQLite and Athena\n",
    "stat_reconclie = pd.concat([stat_select_gl_header_sqlite, stat_select_gl_header_athena], axis=1)\n",
    "\n",
    "#check status if suffix sqlite is match with athena using lambda for each column and row\n",
    "stat_reconclie['invoice_no_receipt_no_status'] = stat_reconclie.apply(lambda x: 'Match' if x['invoice_no_receipt_no_sqlite'] == x['invoice_no_receipt_no'] else 'Mismatch', axis=1)\n",
    "stat_reconclie['payment_method_status'] = stat_reconclie.apply(lambda x: 'Match' if x['payment_method_sqlite'] == x['payment_method'] else 'Mismatch', axis=1)\n",
    "stat_reconclie['status'] = stat_reconclie.apply(lambda x: 'Match' if x['invoice_no_receipt_no_sqlite'] == x['invoice_no_receipt_no'] and x['payment_method_sqlite'] == x['payment_method']  else 'Mismatch', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### export to .csv file with sheets for each type of reconcilation\n",
    "current_date = datetime.now().strftime('%Y%m%d')\n",
    "output_path = rf\"{pwd}/report/{current_date}_gl_header_reconcile.xlsx\"\n",
    "with pd.ExcelWriter(output_path) as writer:  \n",
    "    df_reconcile.to_excel(writer, sheet_name='Data Reconcile', index=False)\n",
    "    stat_reconclie.to_excel(writer, sheet_name='Stat Reconcile', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
